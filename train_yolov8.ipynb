{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37217dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# STEP 1: Setup Environment\n",
    "# ============================\n",
    "!pip install ultralytics roboflow -q\n",
    "\n",
    "from roboflow import Roboflow\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d6e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# STEP 2: Connect Roboflow Dataset\n",
    "# ============================\n",
    "# Get dataset from Roboflow (public dataset example)\n",
    "# Go to https://roboflow.com/datasets, pick one, and click \"Download -> YOLOv8 -> Copy Code\"\n",
    "\n",
    "rf = Roboflow(api_key=\"YOUR_API_KEY\")  # replace with your Roboflow API key\n",
    "project = rf.workspace(\"roboflow-public-datasets\").project(\"blood-cells\")  # example public dataset\n",
    "dataset = project.version(2).download(\"yolov8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856092f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# STEP 3: Train YOLOv8 Model\n",
    "# ============================\n",
    "\n",
    "# Load a pretrained YOLOv8 model\n",
    "# Options:\n",
    "# yolov8n (nano, fastest)\n",
    "# yolov8s (small)\n",
    "# yolov8m (medium)\n",
    "# yolov8l (large)\n",
    "# yolov8x (x-large, most accurate)\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "# Train the model on your dataset\n",
    "model.train(\n",
    "    data=dataset.location + \"/data.yaml\",  # Path to dataset config file; defines train/val image paths and class names\n",
    "    epochs=50,                             # Number of full training passes through the dataset\n",
    "    imgsz=640,                             # Image size (in pixels) YOLO resizes all images to before training\n",
    "    batch=16,                              # Number of images processed per iteration (adjust based on GPU memory)\n",
    "    name=\"yolov8\",                         # Name of this training run; saved under runs/detect/yolov8/\n",
    "    patience=10                            # Early stopping: stop if no validation improvement after 10 epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# STEP 4: Evaluate & Export\n",
    "# ============================\n",
    "# Evaluate model on validation set\n",
    "model.val()\n",
    "\n",
    "# Export the trained YOLOv8 model to different formats for deployment\n",
    "# Common export formats:\n",
    "# - \"pt\"          → PyTorch format (default, used for further YOLOv8 training or inference)\n",
    "# - \"onnx\"        → Open Neural Network Exchange (for interoperability, e.g. OpenCV, TensorRT, etc.)\n",
    "# - \"torchscript\" → For embedding models into C++/mobile environments\n",
    "# - \"engine\"      → TensorRT engine (optimized GPU inference)\n",
    "# - \"coreml\"      → Apple CoreML format (for iOS/macOS deployment)\n",
    "# - \"tflite\"      → TensorFlow Lite (for Android, embedded devices)\n",
    "# - \"pb\"          → TensorFlow SavedModel format\n",
    "# - \"xml\"         → OpenVINO IR format (for Intel hardware)\n",
    "# - \"ncnn\"        → NCNN format (for lightweight inference on edge devices)\n",
    "# - \"edgetpu\"     → TensorFlow EdgeTPU format (for Coral EdgeTPU accelerators)\n",
    "# - \"tfjs\"        → TensorFlow.js (for running in web browsers)\n",
    "\n",
    "model.export(format=\"onnx\")  # Example: export model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5f5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# STEP 5: Test on Sample Image\n",
    "# ============================\n",
    "\n",
    "# Run inference (prediction) using the trained model\n",
    "results = model.predict(\n",
    "    source=dataset.location + \"/test/images\",  # Path to the folder or file to run detection on\n",
    "                                                # Can also be a single image path, video file, webcam index (0), or URL\n",
    "    conf=0.5,                                   # Confidence threshold: ignore detections below this probability\n",
    "    save=True                                   # Save output images/videos with bounding boxes and labels to 'runs/predict/'\n",
    ")\n",
    "\n",
    "# Print detection summary (class names, confidences, bounding boxes, etc.)\n",
    "print(results)\n",
    "# To view saved prediction images:\n",
    "# Files will be stored in runs/predict/ — open that folder in Colab file explorer to preview results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
